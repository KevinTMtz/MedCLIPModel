{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ad690a6",
   "metadata": {},
   "source": [
    "# MedCLIP Captioning Model Dual Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b677242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join, expanduser\n",
    "import collections\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import efficientnet\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppressing tf.hub warnings\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11be2042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset files\n",
    "DATASET_ROOT_DIR = 'datasets'\n",
    "IMAGES_DIR = 'cases_images'\n",
    "DATASET_ANNOTATIONS = 'Dataset_MedPix_V1.xlsx'\n",
    "\n",
    "IMAGES_PATH = os.path.join(DATASET_ROOT_DIR, IMAGES_DIR)\n",
    "ANNOTATION_FILE = os.path.join(DATASET_ROOT_DIR, DATASET_ANNOTATIONS)\n",
    "TFRECORDS_DIR = os.path.join(DATASET_ROOT_DIR, 'tfrecords')\n",
    "\n",
    "# Model Weights\n",
    "MODELS_ROOT_DIR = 'models'\n",
    "MODEL_DIR = 'captioning_model_dual_encoder'\n",
    "VISION_MODEL = 'vision_encoder'\n",
    "TEXT_MODEL = 'text_encoder'\n",
    "\n",
    "MODEL_DIR_PATH = os.path.join(MODELS_ROOT_DIR, MODEL_DIR)\n",
    "VISION_MODEL_PATH = os.path.join(MODEL_DIR_PATH, VISION_MODEL)\n",
    "TEXT_MODEL_PATH = os.path.join(MODEL_DIR_PATH, TEXT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a115a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "TRAIN_SIZE = 30000\n",
    "VALID_SIZE = 6000\n",
    "\n",
    "CAPTIONS_PER_IMAGE = 1\n",
    "IMAGES_PER_FILE = 2000\n",
    "\n",
    "IMAGE_SIZE = (299, 299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cdc887",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_excel(ANNOTATION_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6053b438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download image files\n",
    "if not os.path.exists(IMAGES_PATH):\n",
    "    for index, row in annotations.iterrows():\n",
    "        url = row['Image_URL']\n",
    "        file_name = str(index) + '.jpg'\n",
    "        r = requests.get(url)\n",
    "        filepath = join(DATASET_ROOT_DIR, IMAGES_PATH)\n",
    "\n",
    "        if not os.path.exists(filepath):\n",
    "            os.makedirs(filepath)\n",
    "\n",
    "        filepath = join(filepath, file_name)\n",
    "\n",
    "        if r.status_code == 200:\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "\n",
    "print('Image files are downloaded in: ' + IMAGES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8bb32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_to_caption = collections.defaultdict(list)\n",
    "\n",
    "for index, row in annotations.iterrows():\n",
    "    diagnosis = str(row['Diagnosis'])\n",
    "    caption = str(row['Caption'])\n",
    "    image_path = IMAGES_PATH + \"/\" + str(row['ID']) + \".jpg\"\n",
    "    image_path_to_caption[image_path].append(diagnosis)\n",
    "    image_path_to_caption[image_path].append(caption)\n",
    "\n",
    "image_paths = list(image_path_to_caption.keys())\n",
    "print(f\"Number of images: {len(image_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6432c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_paths = image_paths[:TRAIN_SIZE]\n",
    "num_train_files = int(np.ceil(TRAIN_SIZE / IMAGES_PER_FILE))\n",
    "train_files_prefix = os.path.join(TFRECORDS_DIR, \"train\")\n",
    "\n",
    "valid_image_paths = image_paths[-VALID_SIZE:]\n",
    "num_valid_files = int(np.ceil(VALID_SIZE / IMAGES_PER_FILE))\n",
    "valid_files_prefix = os.path.join(TFRECORDS_DIR, \"valid\")\n",
    "\n",
    "tf.io.gfile.makedirs(TFRECORDS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a1ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def create_example(image_path, caption):\n",
    "    feature = {\n",
    "        \"caption\": bytes_feature(caption.encode()),\n",
    "        \"raw_image\": bytes_feature(tf.io.read_file(image_path).numpy()),\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "def write_tfrecords(file_name, image_paths):\n",
    "    caption_list = []\n",
    "    image_path_list = []\n",
    "    for image_path in image_paths:\n",
    "        captions = image_path_to_caption[image_path][:CAPTIONS_PER_IMAGE]\n",
    "        caption_list.extend(captions)\n",
    "        image_path_list.extend([image_path] * len(captions))\n",
    "\n",
    "    with tf.io.TFRecordWriter(file_name) as writer:\n",
    "        for example_idx in range(len(image_path_list)):\n",
    "            example = create_example(\n",
    "                image_path_list[example_idx], caption_list[example_idx]\n",
    "            )\n",
    "            writer.write(example.SerializeToString())\n",
    "    return example_idx + 1\n",
    "\n",
    "\n",
    "def write_data(image_paths, num_files, files_prefix):\n",
    "    example_counter = 0\n",
    "    for file_idx in tqdm(range(num_files)):\n",
    "        file_name = files_prefix + \"-%02d.tfrecord\" % (file_idx)\n",
    "        start_idx = IMAGES_PER_FILE * file_idx\n",
    "        end_idx = start_idx + IMAGES_PER_FILE\n",
    "        example_counter += write_tfrecords(file_name, image_paths[start_idx:end_idx])\n",
    "    return example_counter\n",
    "\n",
    "\n",
    "train_example_count = write_data(train_image_paths, num_train_files, train_files_prefix)\n",
    "print(f\"{train_example_count} training examples were written to tfrecord files.\")\n",
    "\n",
    "valid_example_count = write_data(valid_image_paths, num_valid_files, valid_files_prefix)\n",
    "print(f\"{valid_example_count} evaluation examples were written to tfrecord files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bacb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    \"caption\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"raw_image\": tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "\n",
    "def read_example(example):\n",
    "    features = tf.io.parse_single_example(example, feature_description)\n",
    "    raw_image = features.pop(\"raw_image\")\n",
    "    features[\"image\"] = tf.image.resize(\n",
    "        tf.image.decode_jpeg(raw_image, channels=3), size=(299, 299)\n",
    "    )\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_dataset(file_pattern, BATCH_SIZE):\n",
    "\n",
    "    return (\n",
    "        tf.data.TFRecordDataset(tf.data.Dataset.list_files(file_pattern))\n",
    "        .map(\n",
    "            read_example,\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "            deterministic=False,\n",
    "        )\n",
    "        .shuffle(BATCH_SIZE * 10)\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        .batch(BATCH_SIZE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe595b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_embeddings(\n",
    "    embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "):\n",
    "    projected_embeddings = layers.Dense(units=projection_dims)(embeddings)\n",
    "    for _ in range(num_projection_layers):\n",
    "        x = tf.nn.gelu(projected_embeddings)\n",
    "        x = layers.Dense(projection_dims)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.Add()([projected_embeddings, x])\n",
    "        projected_embeddings = layers.LayerNormalization()(x)\n",
    "    return projected_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a266976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vision_encoder(\n",
    "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
    "):\n",
    "    # Load the pre-trained Xception model to be used as the base encoder.\n",
    "    xception = keras.applications.Xception(\n",
    "        include_top=False, weights=\"imagenet\", pooling=\"avg\"\n",
    "    )\n",
    "    # Set the trainability of the base encoder.\n",
    "    for layer in xception.layers:\n",
    "        layer.trainable = trainable\n",
    "    # Receive the images as inputs.\n",
    "    inputs = layers.Input(shape=(299, 299, 3), name=\"image_input\")\n",
    "    # Preprocess the input image.\n",
    "    xception_input = tf.keras.applications.xception.preprocess_input(inputs)\n",
    "    # Generate the embeddings for the images using the xception model.\n",
    "    embeddings = xception(xception_input)\n",
    "    # Project the embeddings produced by the model.\n",
    "    outputs = project_embeddings(\n",
    "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "    )\n",
    "    # Create the vision encoder model.\n",
    "    return keras.Model(inputs, outputs, name=\"vision_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8def108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_encoder(\n",
    "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
    "):\n",
    "    # Load the BERT preprocessing module.\n",
    "    preprocess = hub.KerasLayer(\n",
    "        \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2\",\n",
    "        name=\"text_preprocessing\",\n",
    "    )\n",
    "    # Load the pre-trained BERT model to be used as the base encoder.\n",
    "    bert = hub.KerasLayer(\n",
    "        \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\",\n",
    "        \"bert\",\n",
    "    )\n",
    "    # Set the trainability of the base encoder.\n",
    "    bert.trainable = trainable\n",
    "    # Receive the text as inputs.\n",
    "    inputs = layers.Input(shape=(), dtype=tf.string, name=\"text_input\")\n",
    "    # Preprocess the text.\n",
    "    bert_inputs = preprocess(inputs)\n",
    "    # Generate embeddings for the preprocessed text using the BERT model.\n",
    "    embeddings = bert(bert_inputs)[\"pooled_output\"]\n",
    "    # Project the embeddings produced by the model.\n",
    "    outputs = project_embeddings(\n",
    "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "    )\n",
    "    # Create the text encoder model.\n",
    "    return keras.Model(inputs, outputs, name=\"text_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb729528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualEncoder(keras.Model):\n",
    "    def __init__(self, text_encoder, image_encoder, temperature=1.0, **kwargs):\n",
    "        super(DualEncoder, self).__init__(**kwargs)\n",
    "        self.text_encoder = text_encoder\n",
    "        self.image_encoder = image_encoder\n",
    "        self.temperature = temperature\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "    def call(self, features, training=False):\n",
    "        # Place each encoder on a separate GPU (if available).\n",
    "        # TF will fallback on available devices if there are fewer than 2 GPUs.\n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            # Get the embeddings for the captions.\n",
    "            caption_embeddings = text_encoder(features[\"caption\"], training=training)\n",
    "        with tf.device(\"/gpu:1\"):\n",
    "            # Get the embeddings for the images.\n",
    "            image_embeddings = vision_encoder(features[\"image\"], training=training)\n",
    "        return caption_embeddings, image_embeddings\n",
    "\n",
    "    def compute_loss(self, caption_embeddings, image_embeddings):\n",
    "        # logits[i][j] is the dot_similarity(caption_i, image_j).\n",
    "        logits = (\n",
    "            tf.matmul(caption_embeddings, image_embeddings, transpose_b=True)\n",
    "            / self.temperature\n",
    "        )\n",
    "        # images_similarity[i][j] is the dot_similarity(image_i, image_j).\n",
    "        images_similarity = tf.matmul(\n",
    "            image_embeddings, image_embeddings, transpose_b=True\n",
    "        )\n",
    "        # captions_similarity[i][j] is the dot_similarity(caption_i, caption_j).\n",
    "        captions_similarity = tf.matmul(\n",
    "            caption_embeddings, caption_embeddings, transpose_b=True\n",
    "        )\n",
    "        # targets[i][j] = avarage dot_similarity(caption_i, caption_j) and dot_similarity(image_i, image_j).\n",
    "        targets = keras.activations.softmax(\n",
    "            (captions_similarity + images_similarity) / (2 * self.temperature)\n",
    "        )\n",
    "        # Compute the loss for the captions using crossentropy\n",
    "        captions_loss = keras.losses.categorical_crossentropy(\n",
    "            y_true=targets, y_pred=logits, from_logits=True\n",
    "        )\n",
    "        # Compute the loss for the images using crossentropy\n",
    "        images_loss = keras.losses.categorical_crossentropy(\n",
    "            y_true=tf.transpose(targets), y_pred=tf.transpose(logits), from_logits=True\n",
    "        )\n",
    "        # Return the mean of the loss over the batch.\n",
    "        return (captions_loss + images_loss) / 2\n",
    "\n",
    "    def train_step(self, features):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            caption_embeddings, image_embeddings = self(features, training=True)\n",
    "            loss = self.compute_loss(caption_embeddings, image_embeddings)\n",
    "        # Backward pass\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        # Monitor loss\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, features):\n",
    "        caption_embeddings, image_embeddings = self(features, training=False)\n",
    "        loss = self.compute_loss(caption_embeddings, image_embeddings)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47a7a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_encoder = create_vision_encoder(\n",
    "    num_projection_layers=1, projection_dims=256, dropout_rate=0.1\n",
    ")\n",
    "text_encoder = create_text_encoder(\n",
    "    num_projection_layers=1, projection_dims=256, dropout_rate=0.1\n",
    ")\n",
    "dual_encoder = DualEncoder(text_encoder, vision_encoder, temperature=0.05)\n",
    "dual_encoder.compile(\n",
    "    optimizer=tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b39622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot callback\n",
    "class PlotLearning(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback to plot the learning curves of the model during training.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.epochs = []\n",
    "        self.times = []\n",
    "        self.timetaken = 0\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.metrics = {}\n",
    "        for metric in logs:\n",
    "            self.metrics[metric] = []\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.timetaken = tf.timestamp()\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Storing metrics\n",
    "        for metric in logs:\n",
    "            if metric in self.metrics:\n",
    "                self.metrics[metric].append(logs.get(metric))\n",
    "            else:\n",
    "                self.metrics[metric] = [logs.get(metric)]\n",
    "                \n",
    "        # Storing time\n",
    "        self.times.append((tf.timestamp() - self.timetaken) / 60)\n",
    "        self.epochs.append(epoch + 1)\n",
    "        \n",
    "        # Plot accuracy and loss per epoch\n",
    "        metrics = [x for x in logs if 'val' not in x]\n",
    "        \n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        fig.set_facecolor('white')\n",
    "        \n",
    "        subfigs = fig.subfigures(2, 1)\n",
    "        \n",
    "        subplots = subfigs[0].subplots(1, len(metrics))\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "\n",
    "        for i, metric in enumerate(metrics):\n",
    "            metricVarName = 'accuracy' if metric == 'acc' else metric\n",
    "            \n",
    "            subplots[i].plot(range(1, epoch + 2), self.metrics[metric], label=metricVarName)\n",
    "            \n",
    "            if logs['val_' + metric]:\n",
    "                subplots[i].plot(range(1, epoch + 2), self.metrics['val_' + metric], \n",
    "                                 label='val_' + metricVarName)\n",
    "            \n",
    "            metricLabel = 'Accuracy' if metric == 'acc' else 'Loss'\n",
    "            \n",
    "            subplots[i].set_title('Model ' + metricLabel)\n",
    "            subplots[i].set_ylabel(metricLabel)\n",
    "            subplots[i].set_xlabel('Epoch')\n",
    "            subplots[i].legend()\n",
    "            subplots[i].grid()\n",
    "            \n",
    "        # Plot time per epoch\n",
    "        subplots = subfigs[1].subplots(1, 1)\n",
    "        \n",
    "        subplots.set_title('Minutes per epoch - Total time ' + \n",
    "                  str(round(np.sum(self.times) / 60, 2)) + 'hrs')\n",
    "        subplots.set_xlabel('Epoch')\n",
    "        subplots.set_ylabel('Time in minutes')\n",
    "        subplots.plot(self.epochs, self.times)\n",
    "        subplots.grid()\n",
    "        \n",
    "        bboxprops = dict(boxstyle='round', facecolor='white', alpha=0.75)\n",
    "        \n",
    "        for i in range(len(self.epochs)):\n",
    "            j = self.times[i].numpy()\n",
    "            subplots.text(i + 1, j, str(round(j, 2)), bbox=bboxprops)\n",
    "        \n",
    "        plt.tight_layout(rect=[-0.05, 0.05, 1, 0.92])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b02853",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of GPUs: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "print(f\"Number of examples (caption-image pairs): {train_example_count}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Steps per epoch: {int(np.ceil(train_example_count / BATCH_SIZE))}\")\n",
    "\n",
    "train_dataset = get_dataset(os.path.join(TFRECORDS_DIR, \"train-*.tfrecord\"), BATCH_SIZE)\n",
    "valid_dataset = get_dataset(os.path.join(TFRECORDS_DIR, \"valid-*.tfrecord\"), BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2465580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a learning rate scheduler callback.\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2, patience=3\n",
    ")\n",
    "\n",
    "# Create an early stopping callback.\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7ada9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading vision and text encoders...\")\n",
    "\n",
    "vision_encoder = keras.models.load_model(VISION_MODEL_PATH)\n",
    "text_encoder = keras.models.load_model(TEXT_MODEL_PATH)\n",
    "\n",
    "print(\"Vision model loaded from: \" + VISION_MODEL_PATH)\n",
    "print(\"Text model loaded from: \" + TEXT_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55367928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = dual_encoder.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[reduce_lr, early_stopping, PlotLearning()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8120c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving vision and text encoders...\")\n",
    "\n",
    "vision_encoder.save(VISION_MODEL_PATH)\n",
    "text_encoder.save(TEXT_MODEL_PATH)\n",
    "\n",
    "print(\"Models are saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03918723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(image_path):\n",
    "    image_array = tf.image.decode_jpeg(tf.io.read_file(image_path), channels=3)\n",
    "    return tf.image.resize(image_array, (299, 299))\n",
    "\n",
    "\n",
    "print(f\"Generating embeddings for {len(image_paths)} images...\")\n",
    "image_embeddings = vision_encoder.predict(\n",
    "    tf.data.Dataset.from_tensor_slices(image_paths).map(read_image).batch(BATCH_SIZE),\n",
    "    verbose=1,\n",
    ")\n",
    "print(f\"Image embeddings shape: {image_embeddings.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7cbd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matches(image_embeddings, queries, k=9, normalize=True):\n",
    "    # Get the embedding for the query.\n",
    "    query_embedding = text_encoder(tf.convert_to_tensor(queries))\n",
    "    # Normalize the query and the image embeddings.\n",
    "    if normalize:\n",
    "        image_embeddings = tf.math.l2_normalize(image_embeddings, axis=1)\n",
    "        query_embedding = tf.math.l2_normalize(query_embedding, axis=1)\n",
    "    # Compute the dot product between the query and the image embeddings.\n",
    "    dot_similarity = tf.matmul(query_embedding, image_embeddings, transpose_b=True)\n",
    "    # Retrieve top k indices.\n",
    "    results = tf.math.top_k(dot_similarity, k).indices.numpy()\n",
    "    # Return matching image paths.\n",
    "    return [[image_paths[idx] for idx in indices] for indices in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a10161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Brain tumor\"\n",
    "matches = find_matches(image_embeddings, [query], normalize=True)[0]\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(mpimg.imread(matches[i]))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c1edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_top_k_accuracy(image_paths, k=100):\n",
    "    hits = 0\n",
    "    num_batches = int(np.ceil(len(image_paths) / BATCH_SIZE))\n",
    "    for idx in tqdm(range(num_batches)):\n",
    "        start_idx = idx * BATCH_SIZE\n",
    "        end_idx = start_idx + BATCH_SIZE\n",
    "        current_image_paths = image_paths[start_idx:end_idx]\n",
    "        queries = [\n",
    "            image_path_to_caption[image_path][0] for image_path in current_image_paths\n",
    "        ]\n",
    "        result = find_matches(image_embeddings, queries, k)\n",
    "        hits += sum(\n",
    "            [\n",
    "                image_path in matches\n",
    "                for (image_path, matches) in list(zip(current_image_paths, result))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return hits / len(image_paths)\n",
    "\n",
    "\n",
    "print(\"Scoring training data...\")\n",
    "train_accuracy = compute_top_k_accuracy(train_image_paths)\n",
    "print(f\"Train accuracy: {round(train_accuracy * 100, 3)}%\")\n",
    "\n",
    "print(\"Scoring evaluation data...\")\n",
    "eval_accuracy = compute_top_k_accuracy(image_paths[TRAIN_SIZE:])\n",
    "print(f\"Eval accuracy: {round(eval_accuracy * 100, 3)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7ed68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export for web pages\n",
    "vision_encoder.save(\"web_models/vision_encoder.h5\")\n",
    "text_encoder.save(\"web_models/text_encoder.h5\")\n",
    "\n",
    "!tensorflowjs_converter --input_format keras ./web_models/vision_encoder.h5 ./web_models/vision_encoder\n",
    "!tensorflowjs_converter --input_format keras ./web_models/text_encoder.h5 ./web_models/text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6ff08e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
