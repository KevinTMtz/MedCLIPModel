{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "435717ff",
   "metadata": {},
   "source": [
    "# MedCLIP Captioning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa87154",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import efficientnet\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import tensorflow_text as text\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import requests\n",
    "from os.path import join, expanduser\n",
    "\n",
    "from enum import Enum, auto\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "914938cd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SEED = 111\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0232faa4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Database(Enum):\n",
    "    MedPix = auto()\n",
    "    CheXpert = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eda95f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select current database\n",
    "CURRENT_DATABASE = Database.CheXpert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa6c2e72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Dataset files\n",
    "DATASET_ROOT_DIR = 'datasets'\n",
    "\n",
    "DATASET_DIR = ''\n",
    "IMAGES_DIR = ''\n",
    "DATASET_ANNOTATIONS = ''\n",
    "\n",
    "if (CURRENT_DATABASE == Database.MedPix):\n",
    "    DATASET_DIR = 'MedPix'\n",
    "    IMAGES_DIR = 'cases_images'\n",
    "    DATASET_ANNOTATIONS = 'Dataset_MedPix_V2.csv'\n",
    "elif (CURRENT_DATABASE == Database.CheXpert):\n",
    "    DATASET_DIR = 'CheXpert-v1.0-small'\n",
    "    IMAGES_DIR = 'train'\n",
    "    DATASET_ANNOTATIONS = 'Dataset_CheXpert.csv'\n",
    "    \n",
    "IMAGES_PATH = os.path.join(DATASET_ROOT_DIR, DATASET_DIR, IMAGES_DIR)\n",
    "ANNOTATION_FILE = os.path.join(DATASET_ROOT_DIR, DATASET_DIR, DATASET_ANNOTATIONS)\n",
    "\n",
    "# Model Weights\n",
    "MODELS_ROOT_DIR = 'models'\n",
    "MODEL_DIR = 'captioning_model_' + DATASET_DIR\n",
    "WEIGHTS_FILE = 'weights'\n",
    "\n",
    "MODEL_DIR_PATH = os.path.join(MODELS_ROOT_DIR, MODEL_DIR)\n",
    "WEIGHTS_FILE_PATH = os.path.join(MODEL_DIR_PATH, WEIGHTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccb07f4a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Number of captions per image\n",
    "CAPTIONS_PER_IMAGE = 1\n",
    "\n",
    "# Desired image dimensions\n",
    "IMAGE_SIZE = (299, 299)\n",
    "\n",
    "# Vocabulary size\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "# Fixed length allowed for any sequence\n",
    "SEQ_LENGTH = 25\n",
    "\n",
    "# Dimension for the image embeddings and token embeddings\n",
    "EMBED_DIM = 512\n",
    "\n",
    "# Per-layer units in the feed-forward network\n",
    "FF_DIM = 512\n",
    "\n",
    "# Other training parameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a12020f",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b03a9331",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "annotations = pd.read_csv(ANNOTATION_FILE)\n",
    "\n",
    "if (CURRENT_DATABASE == Database.CheXpert):\n",
    "    annotations = annotations[:75000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "943c5563",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image files are downloaded in: datasets/CheXpert-v1.0-small/train\n"
     ]
    }
   ],
   "source": [
    "# Download MedPix image files\n",
    "if (CURRENT_DATABASE == Database.MedPix and not os.path.exists(IMAGES_PATH)):\n",
    "    for index, row in annotations.iterrows():\n",
    "        url = row['Image_URL']\n",
    "        file_name = str(index) + '.jpg'\n",
    "        r = requests.get(url)\n",
    "        filepath = join(root_dir, images_dir)\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "\n",
    "    filepath = join(filepath, file_name)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        with open(filepath, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "\n",
    "print('Image files are downloaded in: ' + IMAGES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb16273f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_captions_data():\n",
    "    image_path_to_caption = collections.defaultdict(list)\n",
    "    \n",
    "    text_data = []\n",
    "\n",
    "    for index, row in annotations.iterrows():\n",
    "        caption = ''\n",
    "        image_path = ''\n",
    "        \n",
    "        if (CURRENT_DATABASE == Database.MedPix):\n",
    "            image_path = IMAGES_PATH + \"/\" + str(row['ID']) + \".jpg\"\n",
    "            \n",
    "            caption = ('<start> ' + 'Modality: ' + str(row['Core_Modality']) + ', ' +\n",
    "                       'Plane: ' + str(row['Plane']) + ', ' +\n",
    "                       'Category: ' + str(row['Category']) + ', ' +\n",
    "                       'Diagnosis: ' + str(row['Caption']) + ' <end>')\n",
    "        elif (CURRENT_DATABASE == Database.CheXpert):\n",
    "            image_path = DATASET_ROOT_DIR + \"/\" + str(row['Path'])\n",
    "            \n",
    "            ap_pa = ''\n",
    "            if str(row['Frontal/Lateral']) == 'Frontal':\n",
    "                ap_pa = 'AP/PA: ' + str(row['AP/PA']) + ', '\n",
    "            \n",
    "            caption = ('<start> ' + 'Frontal/Lateral: ' + str(row['Frontal/Lateral']) + ', ' +\n",
    "                       ap_pa + 'Diagnosis: ' + str(row['Caption']) + ' <end>')\n",
    "        \n",
    "        image_path_to_caption[image_path].append(caption)\n",
    "        \n",
    "        text_data.append(caption)\n",
    "\n",
    "    return image_path_to_caption, text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1fb65b8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_val_split(caption_data, train_size=0.8, shuffle=True):\n",
    "    # 1. Get the list of all image names\n",
    "    all_images = list(caption_data.keys())\n",
    "\n",
    "    # 2. Shuffle if necessary\n",
    "    if shuffle:\n",
    "        np.random.shuffle(all_images)\n",
    "\n",
    "    # 3. Split into training and validation sets\n",
    "    train_size = int(len(caption_data) * train_size)\n",
    "\n",
    "    training_data = {\n",
    "        img_name: caption_data[img_name] for img_name in all_images[:train_size]\n",
    "    }\n",
    "    validation_data = {\n",
    "        img_name: caption_data[img_name] for img_name in all_images[train_size:]\n",
    "    }\n",
    "\n",
    "    # 4. Return the splits\n",
    "    return training_data, validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "825bc80e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  60000\n",
      "Number of validation samples:  15000\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "captions_mapping, text_data = load_captions_data()\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, valid_data = train_val_split(captions_mapping)\n",
    "print(\"Number of training samples: \", len(train_data))\n",
    "print(\"Number of validation samples: \", len(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd2cedd5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ae32e95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
    "strip_chars = strip_chars.replace(\"<\", \"\")\n",
    "strip_chars = strip_chars.replace(\">\", \"\")\n",
    "\n",
    "# To keep caption the structure\n",
    "strip_chars = strip_chars.replace(\":\", \"\")\n",
    "strip_chars = strip_chars.replace(\",\", \"\")\n",
    "strip_chars = strip_chars.replace(\"/\", \"\")\n",
    "\n",
    "vectorization = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQ_LENGTH,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "vectorization.adapt(text_data)\n",
    "\n",
    "# Data augmentation for image data\n",
    "image_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.2),\n",
    "        layers.RandomContrast(0.3),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fac300b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def decode_and_resize(img_path, size=IMAGE_SIZE):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SIZE)\n",
    "    return img\n",
    "\n",
    "\n",
    "def read_train_image(img_path, size=IMAGE_SIZE):\n",
    "    img = decode_and_resize(img_path)\n",
    "    img = image_augmentation(tf.expand_dims(img, 0))[0]\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return img\n",
    "\n",
    "\n",
    "def read_valid_image(img_path, size=IMAGE_SIZE):\n",
    "    img = decode_and_resize(img_path)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return img\n",
    "\n",
    "\n",
    "def make_dataset(images, captions, split=\"train\"):\n",
    "    if split == \"train\":\n",
    "        img_dataset = tf.data.Dataset.from_tensor_slices(images).map(\n",
    "            read_train_image, num_parallel_calls=AUTOTUNE\n",
    "        )\n",
    "    else:\n",
    "        img_dataset = tf.data.Dataset.from_tensor_slices(images).map(\n",
    "            read_valid_image, num_parallel_calls=AUTOTUNE\n",
    "        )\n",
    "\n",
    "    cap_dataset = tf.data.Dataset.from_tensor_slices(captions).map(\n",
    "        vectorization, num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.Dataset.zip((img_dataset, cap_dataset))\n",
    "    dataset = dataset.batch(BATCH_SIZE).shuffle(256).prefetch(AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c6d4f1a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pass the list of images and the list of corresponding captions\n",
    "train_dataset = make_dataset(\n",
    "    list(train_data.keys()), list(train_data.values()), split=\"train\"\n",
    ")\n",
    "\n",
    "valid_dataset = make_dataset(\n",
    "    list(valid_data.keys()), list(valid_data.values()), split=\"valid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7056116b",
   "metadata": {},
   "source": [
    "## Define the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1e2bf2d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_cnn_model():\n",
    "    base_model = efficientnet.EfficientNetB3(\n",
    "        input_shape=(*IMAGE_SIZE, 3), include_top=False, weights=\"imagenet\",\n",
    "    )\n",
    "    # We freeze our feature extractor\n",
    "    base_model.trainable = False\n",
    "    base_model_out = base_model.output\n",
    "    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)\n",
    "    cnn_model = keras.models.Model(base_model.input, base_model_out)\n",
    "    \n",
    "    return cnn_model\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.0\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.dense_1 = layers.Dense(embed_dim, activation=\"relu\")\n",
    "\n",
    "    def call(self, inputs, training, mask=None):\n",
    "        inputs = self.layernorm_1(inputs)\n",
    "        inputs = self.dense_1(inputs)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=None,\n",
    "            training=training,\n",
    "        )\n",
    "        out_1 = self.layernorm_2(inputs + attention_output_1)\n",
    "        return out_1\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_tokens = embedded_tokens * self.embed_scale\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoderBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "        self.ffn_layer_1 = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.ffn_layer_2 = layers.Dense(embed_dim)\n",
    "\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "\n",
    "        self.embedding = PositionalEmbedding(\n",
    "            embed_dim=EMBED_DIM, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE\n",
    "        )\n",
    "        self.out = layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n",
    "\n",
    "        self.dropout_1 = layers.Dropout(0.3)\n",
    "        self.dropout_2 = layers.Dropout(0.5)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, training, mask=None):\n",
    "        inputs = self.embedding(inputs)\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
    "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
    "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=combined_mask,\n",
    "            training=training,\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "            training=training,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        ffn_out = self.ffn_layer_1(out_2)\n",
    "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
    "        ffn_out = self.ffn_layer_2(ffn_out)\n",
    "\n",
    "        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)\n",
    "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
    "        preds = self.out(ffn_out)\n",
    "        return preds\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7ffaab5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(keras.Model):\n",
    "    def __init__(\n",
    "        self, cnn_model, encoder, decoder, num_captions_per_image=CAPTIONS_PER_IMAGE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n",
    "        self.num_captions_per_image = num_captions_per_image\n",
    "\n",
    "    def calculate_loss(self, y_true, y_pred, mask):\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        mask = tf.cast(mask, dtype=loss.dtype)\n",
    "        loss *= mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
    "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
    "        accuracy = tf.math.logical_and(mask, accuracy)\n",
    "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
    "\n",
    "    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):\n",
    "        encoder_out = self.encoder(img_embed, training=training)\n",
    "        batch_seq_inp = batch_seq[:, :-1]\n",
    "        batch_seq_true = batch_seq[:, 1:]\n",
    "        mask = tf.math.not_equal(batch_seq_true, 0)\n",
    "        batch_seq_pred = self.decoder(\n",
    "            batch_seq_inp, encoder_out, training=training, mask=mask\n",
    "        )\n",
    "        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n",
    "        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n",
    "        return loss, acc\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        batch_img, batch_seq = batch_data\n",
    "        batch_loss = 0\n",
    "        batch_acc = 0\n",
    "\n",
    "        # 1. Get image embeddings\n",
    "        img_embed = self.cnn_model(batch_img)\n",
    "\n",
    "        # 2. Pass each of the five captions one by one to the decoder\n",
    "        # along with the encoder outputs and compute the loss as well as accuracy\n",
    "        # for each caption.\n",
    "        for i in range(self.num_captions_per_image):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss, acc = self._compute_caption_loss_and_acc(\n",
    "                    img_embed, batch_seq[:, i, :], training=True\n",
    "                )\n",
    "\n",
    "                # 3. Update loss and accuracy\n",
    "                batch_loss += loss\n",
    "                batch_acc += acc\n",
    "\n",
    "            # 4. Get the list of all the trainable weights\n",
    "            train_vars = (\n",
    "                self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "            )\n",
    "\n",
    "            # 5. Get the gradients\n",
    "            grads = tape.gradient(loss, train_vars)\n",
    "\n",
    "            # 6. Update the trainable weights\n",
    "            self.optimizer.apply_gradients(zip(grads, train_vars))\n",
    "\n",
    "        # 7. Update the trackers\n",
    "        batch_acc /= float(self.num_captions_per_image)\n",
    "        self.loss_tracker.update_state(batch_loss)\n",
    "        self.acc_tracker.update_state(batch_acc)\n",
    "\n",
    "        # 8. Return the loss and accuracy values\n",
    "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "\n",
    "    def test_step(self, batch_data):\n",
    "        batch_img, batch_seq = batch_data\n",
    "        batch_loss = 0\n",
    "        batch_acc = 0\n",
    "\n",
    "        # 1. Get image embeddings\n",
    "        img_embed = self.cnn_model(batch_img)\n",
    "\n",
    "        # 2. Pass each of the five captions one by one to the decoder\n",
    "        # along with the encoder outputs and compute the loss as well as accuracy\n",
    "        # for each caption.\n",
    "        for i in range(self.num_captions_per_image):\n",
    "            loss, acc = self._compute_caption_loss_and_acc(\n",
    "                img_embed, batch_seq[:, i, :], training=False\n",
    "            )\n",
    "\n",
    "            # 3. Update batch loss and batch accuracy\n",
    "            batch_loss += loss\n",
    "            batch_acc += acc\n",
    "\n",
    "        batch_acc /= float(self.num_captions_per_image)\n",
    "\n",
    "        # 4. Update the trackers\n",
    "        self.loss_tracker.update_state(batch_loss)\n",
    "        self.acc_tracker.update_state(batch_acc)\n",
    "\n",
    "        # 5. Return the loss and accuracy values\n",
    "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker, self.acc_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95a8c1fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "cnn_model = get_cnn_model()\n",
    "encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=1)\n",
    "decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=2)\n",
    "caption_model = ImageCaptioningModel(\n",
    "    cnn_model=cnn_model, encoder=encoder, decoder=decoder \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3152702f",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd8b0b49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "cross_entropy = keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction=\"none\"\n",
    ")\n",
    "\n",
    "# EarlyStopping criteria\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58c4507f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler for the optimizer\n",
    "class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, post_warmup_learning_rate, warmup_steps):\n",
    "        super().__init__()\n",
    "        self.post_warmup_learning_rate = post_warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        global_step = tf.cast(step, tf.float32)\n",
    "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "        warmup_progress = global_step / warmup_steps\n",
    "        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress\n",
    "        return tf.cond(\n",
    "            global_step < warmup_steps,\n",
    "            lambda: warmup_learning_rate,\n",
    "            lambda: self.post_warmup_learning_rate,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c8c0cb0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot callback\n",
    "class PlotLearning(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback to plot the learning curves of the model during training.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.epochs = []\n",
    "        self.times = []\n",
    "        self.timetaken = 0\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.metrics = {}\n",
    "        for metric in logs:\n",
    "            self.metrics[metric] = []\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.timetaken = tf.timestamp()\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Storing metrics\n",
    "        for metric in logs:\n",
    "            if metric in self.metrics:\n",
    "                self.metrics[metric].append(logs.get(metric))\n",
    "            else:\n",
    "                self.metrics[metric] = [logs.get(metric)]\n",
    "                \n",
    "        # Storing time\n",
    "        self.times.append((tf.timestamp() - self.timetaken) / 60)\n",
    "        self.epochs.append(epoch + 1)\n",
    "        \n",
    "        # Plot accuracy and loss per epoch\n",
    "        metrics = [x for x in logs if 'val' not in x]\n",
    "        \n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        fig.set_facecolor('white')\n",
    "        \n",
    "        subfigs = fig.subfigures(2, 1)\n",
    "        \n",
    "        subplots = subfigs[0].subplots(1, len(metrics))\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "\n",
    "        for i, metric in enumerate(metrics):\n",
    "            metricVarName = 'accuracy' if metric == 'acc' else metric\n",
    "            \n",
    "            subplots[i].plot(range(1, epoch + 2), self.metrics[metric], label=metricVarName)\n",
    "            \n",
    "            if logs['val_' + metric]:\n",
    "                subplots[i].plot(range(1, epoch + 2), self.metrics['val_' + metric], \n",
    "                                 label='val_' + metricVarName)\n",
    "            \n",
    "            metricLabel = 'Accuracy' if metric == 'acc' else 'Loss'\n",
    "            \n",
    "            subplots[i].set_title('Model ' + metricLabel)\n",
    "            subplots[i].set_ylabel(metricLabel)\n",
    "            subplots[i].set_xlabel('Epoch')\n",
    "            subplots[i].legend()\n",
    "            subplots[i].grid()\n",
    "            \n",
    "        # Plot time per epoch\n",
    "        subplots = subfigs[1].subplots(1, 1)\n",
    "        \n",
    "        subplots.set_title('Minutes per epoch - Total time ' + \n",
    "                  str(round(np.sum(self.times) / 60, 2)) + 'hrs')\n",
    "        subplots.set_xlabel('Epoch')\n",
    "        subplots.set_ylabel('Time in minutes')\n",
    "        subplots.plot(self.epochs, self.times)\n",
    "        subplots.grid()\n",
    "        \n",
    "        bboxprops = dict(boxstyle='round', facecolor='white', alpha=0.75)\n",
    "        \n",
    "        for i in range(len(self.epochs)):\n",
    "            j = self.times[i].numpy()\n",
    "            subplots.text(i + 1, j, str(round(j, 2)), bbox=bboxprops)\n",
    "        \n",
    "        plt.tight_layout(rect=[-0.05, 0.05, 1, 0.92])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81e40e23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a learning rate schedule\n",
    "num_train_steps = len(train_dataset) * EPOCHS\n",
    "num_warmup_steps = num_train_steps // 15\n",
    "lr_schedule = LRSchedule(post_warmup_learning_rate=1e-3, warmup_steps=num_warmup_steps)\n",
    "\n",
    "# Compile the model\n",
    "caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4968aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the model weights\n",
    "print('Loading model weights...')\n",
    "\n",
    "caption_model.load_weights(WEIGHTS_FILE_PATH)\n",
    "\n",
    "print('Model weights loaded from: ' + WEIGHTS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79029f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = caption_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[early_stopping, PlotLearning()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3e9524",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save the model weights\n",
    "print(\"Saving model weights...\")\n",
    "\n",
    "caption_model.save_weights(WEIGHTS_FILE_PATH, save_format='tf')\n",
    "\n",
    "print('Weights are saved in: ' + WEIGHTS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc6c45",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefe72bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vocab = vectorization.get_vocabulary()\n",
    "index_lookup = dict(zip(range(len(vocab)), vocab))\n",
    "max_decoded_sentence_length = SEQ_LENGTH - 1\n",
    "valid_images = list(valid_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1712a8c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_caption(path, plotImage):\n",
    "    # Read the image from the disk\n",
    "    sample_img = read_valid_image(path)\n",
    "    img = sample_img.numpy().clip(0, 255).astype(np.uint8)\n",
    "    \n",
    "    if plotImage:\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    # Pass the image to the CNN\n",
    "    img = tf.expand_dims(sample_img, 0)\n",
    "    img = caption_model.cnn_model(img)\n",
    "\n",
    "    # Pass the image features to the Transformer encoder\n",
    "    encoded_img = caption_model.encoder(img, training=False)\n",
    "\n",
    "    # Generate the caption using the Transformer decoder\n",
    "    decoded_caption = \"<start> \"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_caption = vectorization([decoded_caption])[:, :-1]\n",
    "        mask = tf.math.not_equal(tokenized_caption, 0)\n",
    "        predictions = caption_model.decoder(\n",
    "            tokenized_caption, encoded_img, training=False, mask=mask\n",
    "        )\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = index_lookup[sampled_token_index]\n",
    "        if sampled_token == \" <end>\":\n",
    "            break\n",
    "        decoded_caption += \" \" + sampled_token\n",
    "\n",
    "    decoded_caption = decoded_caption.replace(\"<start> \", \"\")\n",
    "    decoded_caption = decoded_caption.replace(\" <end>\", \"\").strip()\n",
    "    \n",
    "    return decoded_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3c1272",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_random_caption():\n",
    "    image_path = np.random.choice(valid_images)\n",
    "    print(\"Image Path: \" + image_path)\n",
    "    \n",
    "    print(generate_caption(image_path, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c13bb86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def generate_caption_with_path(path):\n",
    "    print(\"Image Path: \" + path)\n",
    "    \n",
    "    print(generate_caption(path, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b838833",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_random_caption()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f38adf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_random_caption()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a73abd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generate_random_caption()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce78122b",
   "metadata": {},
   "source": [
    "## Evaluate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889f5d35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# BLEU & ROUGE Score\n",
    "def evaluate_bleu_and_rouge_score(image_paths):\n",
    "    gramScores = [0, 0, 0, 0]\n",
    "    f_p_r_measures = [0, 0, 0]\n",
    "    \n",
    "    bleuWeights = [(1, 0, 0, 0), (0, 1, 0, 0), (1, 0, 1, 0), (0, 0, 0, 1)]\n",
    "    for path, captions in tqdm(image_paths.items()):\n",
    "        test_caption = generate_caption(path, False).split()\n",
    "        hypo_captions = np.array(captions, copy=True).tolist()\n",
    "        \n",
    "        for i in range(len(hypo_captions)):\n",
    "            hypo_captions[i] = hypo_captions[i].replace(\"<start> \", \"\")\n",
    "            hypo_captions[i] = hypo_captions[i].replace(\" <end>\", \"\")\n",
    "            hypo_captions[i] = hypo_captions[i].split()\n",
    "        \n",
    "        # BLEU Score\n",
    "        for i in range(len(gramScores)):\n",
    "            gramScores[i] += sentence_bleu(hypo_captions, test_caption, weights=bleuWeights[i])\n",
    "        \n",
    "        # ROUGE Score\n",
    "        result = text.metrics.rouge_l(tf.ragged.constant(hypo_captions), \n",
    "                                    tf.ragged.constant([test_caption]))\n",
    "        \n",
    "        f_p_r_measures[0] += result.f_measure.numpy()[0]\n",
    "        f_p_r_measures[1] += result.p_measure.numpy()[0]\n",
    "        f_p_r_measures[2] += result.r_measure.numpy()[0]\n",
    "    \n",
    "    for i in range(len(gramScores)):\n",
    "        print('BLEU Score ' + str(i+1) + ' gram: ' + str(gramScores[i] / len(image_paths)))\n",
    "    \n",
    "    print()\n",
    "    print('ROUGE F Score: ' + str(f_p_r_measures[0] / len(image_paths)))\n",
    "    print('ROUGE P Score: ' + str(f_p_r_measures[1] / len(image_paths)))\n",
    "    print('ROUGE R Score: ' + str(f_p_r_measures[2] / len(image_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f3b289",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluate valid data BLEU & ROUGE score\n",
    "evaluate_bleu_and_rouge_score(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88618998",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluate train data BLEU & ROUGE score\n",
    "evaluate_bleu_and_rouge_score(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c4a9c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
