{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://becasparatodos.com/wp-content/uploads/2017/01/tec-de-monterrey-maestr%C3%ADas.jpg\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Campus Ciudad de México, \n",
    "Escuela Ingeniería y Ciencias, \n",
    "Computación</center></h2>\n",
    "\n",
    "<h4><center>Course: TC3020.1 (Machine Learning)</center>\n",
    "<center>Professor: Mauricio Rosales Rivera</center>\n",
    "<center>Final Project - Evaluation</center></h4>\n",
    "    \n",
    "<h4><center>Date: November 12, 2021</center></h4>\n",
    "<h4><center><font color=\"red\">Submit: December, 2021</font></center></h4>\n",
    "\n",
    "---\n",
    "\n",
    "<h4>Grade: </h4>\n",
    "\n",
    "---\n",
    "\n",
    "<h4><center>Student's information</center></h4>\n",
    "<h4>Names: Juan Sebastián Rodríguez Galarza</h4>\n",
    "<h4>Students ID: A01656159</h4>    \n",
    "<h4>Github: https://github.com/SebasRod23</h4>\n",
    "<br>\n",
    "<h4>Names: Kevin Torres Martínez</h4>    \n",
    "<h4>Students ID: A01656257</h4>    \n",
    "<h4>Github: https://github.com/KevinTMtz</h4>\n",
    "<br>\n",
    "<h4>Names: Aldo Fernando Ortiz Mejía</h4>    \n",
    "<h4>Students ID: A01654725</h4>    \n",
    "<h4>Github: https://github.com/kilodecarnitas</h4>\n",
    "<br>\n",
    "<h4>Names: Gerardo Arturo Miranda Godoy</h4>    \n",
    "<h4>Students ID: A01338074</h4>    \n",
    "<h4>Github: https://github.com/garturom</h4>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Apegándome al Código de Ética de los Estudiantes del Tecnológico de Monterrey, me comprometo a que mi actuación en este proyecto esté regida por la honestidad académica. En congruencia con el compromiso adquirido al firmar dicho código, realizaré este proyecto de forma honesta y personal, para reflejar, a través de él, mi conocimiento y aceptar, posteriormente, la evaluación obtenida.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"brown\">Proyecto Final - Aprendizaje Máquina</font>\n",
    "\n",
    "---\n",
    "\n",
    "Indicaciones: El proyecto seleccionado será desarrollado siguiendo ciertas condiciones. Deberán seguir la estructura recomendada (pero podrán cambiar títulos, es sólo una sugerencia) y podrán añadir secciones en caso de que ser necesario.\n",
    "\n",
    "* La fecha límite de entrega de la notebook será: 2021.\n",
    "* Deberán realizar una exposición (entre 10 y 20 minutos a lo mucho), donde proporcionen una explicación acerca de la metodología implementada y los resultados obtenidos.\n",
    "\n",
    "Esta notebook deberá contener el nombre completo, matrícula y el link correspondiente al GitHub de cada integrante.\n",
    "En caso de no tener la información aquí solicitada, no se evaluará el proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"general\"></a>\n",
    "### Contenido\n",
    "\n",
    "-----\n",
    "\n",
    "1. [Objetivo](#a)\n",
    "\n",
    "2. [Exploración de Datos](#b)\n",
    "    * Detección de valores faltantes\n",
    "    * Agregando valores\n",
    "    * Visualizando datos\n",
    "    * Preprocesamiento de datos\n",
    "        * Reducción de dimensiones\n",
    "        \n",
    "-----\n",
    "3. [Selección de modelos](#c)\n",
    "    * Aprendizaje No Supervisado / Supervisado\n",
    "        * Selección de modelo\n",
    "        * Selección de hiperparámetros\n",
    "        * Entrenamiento\n",
    "        * Prueba\n",
    "        * Resultados\n",
    "-----\n",
    "\n",
    "4. [Conclusiones](#d)\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"a\"></a>\n",
    "### 1. Objetivo\n",
    "\n",
    "---\n",
    "[Regresar a contenido](#general)\n",
    "\n",
    "Intentar presentar un enfoque completo para modelar problemas, que va desde el análisis exploratorio de datos hasta la aplicación de técnicas de aprendizaje supervisado y no supervisado a nuestros datos.\n",
    "\n",
    "El contenido de esta notebook está dirigido principalmente para entender mejor las etapas que se realizan en los problemas de Ciencia de Datos y Aprendizaje Máquina (y posiblemente en Aprendizaje Profundo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las grandes promesas del siglo es la aplicación de la inteligencia artificial (IA) en nuestra vida diaria para apoyar y mejorar nuestra vida. Los macrodatos han llegado para acelerar el proceso de transición y modernizar la industria, incluida la medicina, en particular la radiología. La interpretación de imágenes médicas ha ganado interés a medida que surgen más datos etiquetados y el desarrollo de modelos se vuelve más fácil. La predicción precisa de enfermedades es un desafío que ha ganado relevancia ya que actualmente se están desarrollando descriptores médicos para estudios de radiología, se les da una imagen y a cambio se deben dar una descripción. En este estudio, implementamos una arquitectura de subtítulos de imágenes usando CNN, codificadores y decodificadores basados en transformadores con dos conjuntos de datos de acceso abierto que fueron previamente curados. El modelo propuesto puede devolver una descripción en lenguaje natural si se le provee una imagen médica. Se midieron la pérdida, precisión y tiempo por época, así como métricas BLEU y ROUGE para evaluar su desempeño. El modelo ha demostrado ser funcional, no obstante, se deben realizar mejoras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"b\"></a>\n",
    "### 2. Exploración de Datos\n",
    "\n",
    "---\n",
    "[Regresar a contenido](#general)\n",
    "\n",
    "En esta sección se trata de realizar una breve explicación del conjunto de datos a utilizar. Así como tener un orden al momento de importar librerías, mostrar gráficos del EDA y preprocesamiento de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MedPix es una base de datos de imágenes médicas en línea de acceso abierto y gratuito proporcionada por “The National Library of Medicine”. Fue desarrollado originalmente por los Departamentos de Radiología e Informática Biomédica de la Uniformed Services University, Bethesda, Maryland, Estados Unidos. Esta colección de imágenes médicas está organizada, aprobada, seleccionada y revisada por pares por un Panel Editorial. Categoriza y clasifica la imagen y los datos de los pacientes para cada uno de los múltiples subconjuntos de aplicaciones de imágenes, estos subconjuntos incluyen radiología, medicina nuclear, patología, oftalmología, etc. El material proporcionado por el conjunto de datos incluye más de 12.000 escenarios de casos de pacientes, 9.000 temas y casi 59.000 imágenes y se pueden buscar por síntomas y signos del paciente, diagnóstico, sistema de órganos, modalidad de imagen y descripción de la imagen, palabras clave, autores contribuyentes y muchas más opciones de búsqueda.\n",
    "Las categorías y subconjuntos se eligieron con el apoyo y el asesoramiento de radiólogos, expertos en imágenes e ingenieros familiarizados con estas tecnologías. Las columnas seleccionadas fueron: modalidad de la imagen, plano, contraste y diagnóstico. Con estas etiquetas se filtró la información y también se dejaron fuera las imágenes no relacionadas. Al principio, MedPix se utilizó para entrenar el modelo mediante ultrasonido, resonancia magnética, tomografía computarizada, rayos X y medicina nuclear, sin embargo, para las siguientes pruebas todas las modalidades excepto rayos X. Se omitieron las imágenes de TC y RM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CheXpert es otro gran dataset público que contiene 224,316 radiografías de tórax de 65,250 pacientes. Las imágenes se obtuvieron del Hospital de Stanford junto con su informe de radiología. Para cada informe, la presencia de 14 observaciones se etiquetó como positivas (1), negativas (0) o inciertas (-1); por lo que se necesitó de una transformación de datos para obtener una leyenda en lenguaje natural para el modelo. La transformación consistió en la concatenación de todos los diagnósticos positivos como una sola oración con una coma de separación entre los hallazgos. Estos datos se tuvieron en cuenta para entrenamiento adicional debido a su tamaño, modalidad de imagen y confiabilidad. \n",
    "Nota: Para adquirir este dataset, se debe registrar y solicitar a la Universidad de Stamford. La respuesta se recibe casi de inmediato con un enlace para descargar el conjunto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preparación de los datos\n",
    "\n",
    "---\n",
    "[Regresar a contenido](#general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import efficientnet\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import tensorflow_text as text\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import requests\n",
    "from os.path import join, expanduser\n",
    "\n",
    "from enum import Enum, auto\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "import itertools\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 111\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Database(Enum):\n",
    "    MedPix = auto()\n",
    "    CheXpert = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select current database\n",
    "CURRENT_DATABASE = Database.CheXpert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos las constantes respecto al path del dataset seleccionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset files\n",
    "DATASET_ROOT_DIR = 'datasets'\n",
    "\n",
    "DATASET_DIR = ''\n",
    "IMAGES_DIR = ''\n",
    "DATASET_ANNOTATIONS = ''\n",
    "\n",
    "if (CURRENT_DATABASE == Database.MedPix):\n",
    "    DATASET_DIR = 'MedPix'\n",
    "    IMAGES_DIR = 'cases_images'\n",
    "    DATASET_ANNOTATIONS = 'Dataset_MedPix_V2.csv'\n",
    "elif (CURRENT_DATABASE == Database.CheXpert):\n",
    "    DATASET_DIR = 'CheXpert-v1.0-small'\n",
    "    IMAGES_DIR = 'train'\n",
    "    DATASET_ANNOTATIONS = 'Dataset_CheXpert.csv'\n",
    "    \n",
    "IMAGES_PATH = os.path.join(DATASET_ROOT_DIR, DATASET_DIR, IMAGES_DIR)\n",
    "ANNOTATION_FILE = os.path.join(DATASET_ROOT_DIR, DATASET_DIR, DATASET_ANNOTATIONS)\n",
    "\n",
    "# Model Weights\n",
    "MODELS_ROOT_DIR = 'models'\n",
    "MODEL_DIR = 'captioning_model_' + DATASET_DIR\n",
    "WEIGHTS_FILE = 'weights'\n",
    "\n",
    "MODEL_DIR_PATH = os.path.join(MODELS_ROOT_DIR, MODEL_DIR)\n",
    "WEIGHTS_FILE_PATH = os.path.join(MODEL_DIR_PATH, WEIGHTS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos el dataset de CheXpert (columnas etiquetadas a una sola columna con los nombres de las columnas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_csv(os.path.join(DATASET_ROOT_DIR, DATASET_DIR, 'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "        'No Finding',\n",
    "        'Enlarged Cardiomediastinum', \n",
    "        'Cardiomegaly', \n",
    "        'Lung Opacity',\n",
    "        'Lung Lesion',\n",
    "        'Edema',\n",
    "        'Consolidation',\n",
    "        'Pneumonia',\n",
    "        'Atelectasis',\n",
    "        'Pneumothorax',\n",
    "        'Pleural Effusion',\n",
    "        'Pleural Other',\n",
    "        'Fracture',\n",
    "       ]\n",
    "\n",
    "labels = {\n",
    "    1: \"Positive\",\n",
    "    0: \"Negative\",\n",
    "    -1: \"Uncertain\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformData(row):\n",
    "    caption = ''\n",
    "    \n",
    "    if (row['No Finding'] == 1):\n",
    "        return 'No Finding'\n",
    "    \n",
    "    for columnVal in columns:\n",
    "        rowValue = row[columnVal]\n",
    "        \n",
    "        separator = ', ' if len(caption) > 0 else ''\n",
    "        \n",
    "        if rowValue == 0 or rowValue == 1:\n",
    "            # caption += separator + labels[rowValue] + ' ' + columnVal\n",
    "            caption += separator + columnVal\n",
    "        else:\n",
    "            caption = caption + ''\n",
    "    \n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newAnnotations = annotations.copy()\n",
    "\n",
    "newAnnotations[['Caption']] = annotations[columns].apply(lambda row: transformData(row), axis=1)\n",
    "newAnnotations = newAnnotations.drop(columns, axis = 1)\n",
    "newAnnotations = newAnnotations.drop(['Support Devices'], axis = 1)\n",
    "newAnnotations = newAnnotations[newAnnotations['Caption'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newAnnotations.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newAnnotations.to_csv(ANNOTATION_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = pd.read_csv(ANNOTATION_FILE)\n",
    "\n",
    "if (CURRENT_DATABASE == Database.CheXpert):\n",
    "    annotations = annotations.sample(n=75000, random_state=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a MexPix, no es necesario procesar el dataset, únicamente descargar las imágenes desde los URLs presentes en el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MedPix image files\n",
    "if (CURRENT_DATABASE == Database.MedPix and not os.path.exists(IMAGES_PATH)):\n",
    "    for index, row in annotations.iterrows():\n",
    "        url = row['Image_URL']\n",
    "        file_name = str(index) + '.jpg'\n",
    "        r = requests.get(url)\n",
    "        filepath = join(root_dir, images_dir)\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "\n",
    "    filepath = join(filepath, file_name)\n",
    "\n",
    "    if r.status_code == 200:\n",
    "        with open(filepath, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "\n",
    "print('Image files are downloaded in: ' + IMAGES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualización general de los datasets posterior a su preprocesamiento\n",
    "\n",
    "---\n",
    "[Regresar a contenido](#general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medpix_df = pd.read_csv('datasets/MedPix/Dataset_MedPix_V2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only essential columns\n",
    "medpix_df = medpix_df[['ID', 'Image_URL', 'Caption', 'Plane', 'Core_Modality', 'Category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medpix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(medpix_df, x='Core_Modality', color='Core_Modality').update_xaxes(categoryorder='total descending')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(medpix_df, x='Plane', color='Plane').update_xaxes(categoryorder='total descending')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(medpix_df, x='Category').update_xaxes(categoryorder='total descending')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chexpert_df = pd.read_csv('datasets/CheXpert-v1.0-small/Dataset_CheXpert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only essential columns\n",
    "chexpert_df = chexpert_df[['Path', 'Frontal/Lateral', 'AP/PA', 'Caption']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chexpert_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(chexpert_df, x='Frontal/Lateral', color='Frontal/Lateral').update_xaxes(categoryorder='total descending')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(chexpert_df, x='AP/PA').update_xaxes(categoryorder='total descending')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(chexpert_df, x='Caption').update_xaxes(categoryorder='total descending')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"c\"></a>\n",
    "### 3. Selección de modelos\n",
    "\n",
    "---\n",
    "[Regresar a contenido](#general)\n",
    "\n",
    "En esta sección se trata de realizar una breve explicación de la metodología de aprendizaje automático. En caso de aplicar un **pipeline** de aprendizaje no supervisado y / o supervisado, que tenga un orden claro y expliquen el porqué de su aplicación con lo que han percibido de sus datos. \n",
    "\n",
    "El modelo seleccionado, qué parámetros o hiperparámetros eligieron, el porqué entrenaron con cierto tamaño de muestra y los resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizó una arquitectura de CNN-Encoder-Decoder. Primero, se cambió el tamaño de las imágenes obtenidas de los conjuntos de datos (299, 299). Una vez preprocesado y dado como entrada a la red neuronal convolucional EfficentNetB3, que escala uniformemente todas las dimensiones de la red (profundidad / ancho / resolución) mediante el uso de un conjunto de coeficientes fijos. Es necesario usarlo porque si la imagen de entrada es más grande, entonces la red necesita más capas para poder aumentar el campo receptivo y más canales para capturar patrones más finos en la imagen. Como resultado de la aplicación de la CNN, que se entrenó previamente con Imagenet y EfficientNetB3, el modelo toma las características de la imagen y usa un codificador basado en Transformer con atención de múltiples cabezas. En esta etapa, cada característica de la imagen se ha asociado con su importancia, calculada como una función de probabilidad. Luego, la representación de la característica con su correspondiente probabilidad (importancia) se vectoriza y se alimenta al decodificador. Este proceso ha demostrado producir mejores resultados ya que el modelo se enfoca en los rasgos característicos de cada imagen. El captioning comienza proveyendo un token de inicio y la imagen al decodificador, con esto el decodificador inicia un ciclo de iteraciones, donde en cada iteración el modelo genera un vector de probabilidades de las palabras que coinciden mejor y agrega la de mayor probabilidad, considerando el estado actual del caption producido y la característica de la imagen que está resaltada con la máscara de atención. De esta manera, el decodificador agrega nuevas palabras al título de salida hasta que el modelo predice el token final o se haya alcanzado la longitud máxima del título generado. Una vez realizado este proceso, se devuelve el título generado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos constantes generales del modelo, las cuales se seleccionaron de esta manera debido a que fueron las más óptimas debido a la capacidad de cómputo con el que contabamos y la cantidad y diversidad de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of captions per image\n",
    "CAPTIONS_PER_IMAGE = 1\n",
    "\n",
    "# Desired image dimensions\n",
    "IMAGE_SIZE = (299, 299)\n",
    "\n",
    "# Vocabulary size\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "# Fixed length allowed for any sequence\n",
    "SEQ_LENGTH = 25\n",
    "\n",
    "# Dimension for the image embeddings and token embeddings\n",
    "EMBED_DIM = 512\n",
    "\n",
    "# Per-layer units in the feed-forward network\n",
    "FF_DIM = 512\n",
    "\n",
    "# Other training parameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apartir del dataset que se haya cargado, se genera un diccionario que mapea el path de las imágenes con su caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_captions_data():\n",
    "    image_path_to_caption = collections.defaultdict(list)\n",
    "    \n",
    "    text_data = []\n",
    "\n",
    "    for index, row in annotations.iterrows():\n",
    "        caption = ''\n",
    "        image_path = ''\n",
    "        \n",
    "        if (CURRENT_DATABASE == Database.MedPix):\n",
    "            image_path = IMAGES_PATH + \"/\" + str(row['ID']) + \".jpg\"\n",
    "            \n",
    "            caption = ('<start> ' + 'Modality: ' + str(row['Core_Modality']) + ', ' +\n",
    "                       'Plane: ' + str(row['Plane']) + ', ' +\n",
    "                       'Category: ' + str(row['Category']) + ', ' +\n",
    "                       'Diagnosis: ' + str(row['Caption']) + ' <end>')\n",
    "        elif (CURRENT_DATABASE == Database.CheXpert):\n",
    "            image_path = DATASET_ROOT_DIR + \"/\" + str(row['Path'])\n",
    "            \n",
    "            ap_pa = ''\n",
    "            if str(row['Frontal/Lateral']) == 'Frontal':\n",
    "                ap_pa = 'AP/PA: ' + str(row['AP/PA']) + ', '\n",
    "            \n",
    "            caption = ('<start> ' + 'Frontal/Lateral: ' + str(row['Frontal/Lateral']) + ', ' +\n",
    "                       ap_pa + 'Diagnosis: ' + str(row['Caption']) + ' <end>')\n",
    "        \n",
    "        image_path_to_caption[image_path].append(caption)\n",
    "        \n",
    "        text_data.append(caption)\n",
    "\n",
    "    return image_path_to_caption, text_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se divide el dataset para entrenar y probar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(caption_data, train_size=0.8, shuffle=True):\n",
    "    # 1. Get the list of all image names\n",
    "    all_images = list(caption_data.keys())\n",
    "\n",
    "    # 2. Shuffle if necessary\n",
    "    if shuffle:\n",
    "        np.random.shuffle(all_images)\n",
    "\n",
    "    # 3. Split into training and validation sets\n",
    "    train_size = int(len(caption_data) * train_size)\n",
    "\n",
    "    training_data = {\n",
    "        img_name: caption_data[img_name] for img_name in all_images[:train_size]\n",
    "    }\n",
    "    validation_data = {\n",
    "        img_name: caption_data[img_name] for img_name in all_images[train_size:]\n",
    "    }\n",
    "\n",
    "    # 4. Return the splits\n",
    "    return training_data, validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "captions_mapping, text_data = load_captions_data()\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, valid_data = train_val_split(captions_mapping)\n",
    "print(\"Number of training samples: \", len(train_data))\n",
    "print(\"Number of validation samples: \", len(valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se aplica una estandarización a los captions del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
    "strip_chars = strip_chars.replace(\"<\", \"\")\n",
    "strip_chars = strip_chars.replace(\">\", \"\")\n",
    "\n",
    "# To keep caption the structure\n",
    "strip_chars = strip_chars.replace(\":\", \"\")\n",
    "strip_chars = strip_chars.replace(\",\", \"\")\n",
    "strip_chars = strip_chars.replace(\"/\", \"\")\n",
    "\n",
    "vectorization = TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=SEQ_LENGTH,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "vectorization.adapt(text_data)\n",
    "\n",
    "# Data augmentation for image data\n",
    "image_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.2),\n",
    "        layers.RandomContrast(0.3),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen diferentes funciones que nos ayudaran a transformar las imágenes que se carguen para que al momento de introducirlas al modelo todas tengan las mismas características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_and_resize(img_path, size=IMAGE_SIZE):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMAGE_SIZE)\n",
    "    return img\n",
    "\n",
    "\n",
    "def read_train_image(img_path, size=IMAGE_SIZE):\n",
    "    img = decode_and_resize(img_path)\n",
    "    img = image_augmentation(tf.expand_dims(img, 0))[0]\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return img\n",
    "\n",
    "\n",
    "def read_valid_image(img_path, size=IMAGE_SIZE):\n",
    "    img = decode_and_resize(img_path)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return img\n",
    "\n",
    "\n",
    "def make_dataset(images, captions, split=\"train\"):\n",
    "    if split == \"train\":\n",
    "        img_dataset = tf.data.Dataset.from_tensor_slices(images).map(\n",
    "            read_train_image, num_parallel_calls=AUTOTUNE\n",
    "        )\n",
    "    else:\n",
    "        img_dataset = tf.data.Dataset.from_tensor_slices(images).map(\n",
    "            read_valid_image, num_parallel_calls=AUTOTUNE\n",
    "        )\n",
    "\n",
    "    cap_dataset = tf.data.Dataset.from_tensor_slices(captions).map(\n",
    "        vectorization, num_parallel_calls=AUTOTUNE\n",
    "    )\n",
    "\n",
    "    dataset = tf.data.Dataset.zip((img_dataset, cap_dataset))\n",
    "    dataset = dataset.batch(BATCH_SIZE).shuffle(256).prefetch(AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the list of images and the list of corresponding captions\n",
    "train_dataset = make_dataset(\n",
    "    list(train_data.keys()), list(train_data.values()), split=\"train\"\n",
    ")\n",
    "\n",
    "valid_dataset = make_dataset(\n",
    "    list(valid_data.keys()), list(valid_data.values()), split=\"valid\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Definición del modelo\n",
    "\n",
    "---\n",
    "[Regresar a contenido](#general)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se define el modelo CNN, el encoder y el decoder de nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn_model():\n",
    "    base_model = efficientnet.EfficientNetB3(\n",
    "        input_shape=(*IMAGE_SIZE, 3), include_top=False, weights=\"imagenet\",\n",
    "    )\n",
    "    # We freeze our feature extractor\n",
    "    base_model.trainable = False\n",
    "    base_model_out = base_model.output\n",
    "    base_model_out = layers.Reshape((-1, base_model_out.shape[-1]))(base_model_out)\n",
    "    cnn_model = keras.models.Model(base_model.input, base_model_out)\n",
    "    \n",
    "    return cnn_model\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.0\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.dense_1 = layers.Dense(embed_dim, activation=\"relu\")\n",
    "\n",
    "    def call(self, inputs, training, mask=None):\n",
    "        inputs = self.layernorm_1(inputs)\n",
    "        inputs = self.dense_1(inputs)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=None,\n",
    "            training=training,\n",
    "        )\n",
    "        out_1 = self.layernorm_2(inputs + attention_output_1)\n",
    "        return out_1\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_tokens = embedded_tokens * self.embed_scale\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "\n",
    "class TransformerDecoderBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "        )\n",
    "        self.ffn_layer_1 = layers.Dense(ff_dim, activation=\"relu\")\n",
    "        self.ffn_layer_2 = layers.Dense(embed_dim)\n",
    "\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "\n",
    "        self.embedding = PositionalEmbedding(\n",
    "            embed_dim=EMBED_DIM, sequence_length=SEQ_LENGTH, vocab_size=VOCAB_SIZE\n",
    "        )\n",
    "        self.out = layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n",
    "\n",
    "        self.dropout_1 = layers.Dropout(0.3)\n",
    "        self.dropout_2 = layers.Dropout(0.5)\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, training, mask=None):\n",
    "        inputs = self.embedding(inputs)\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
    "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
    "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=combined_mask,\n",
    "            training=training,\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "            training=training,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        ffn_out = self.ffn_layer_1(out_2)\n",
    "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
    "        ffn_out = self.ffn_layer_2(ffn_out)\n",
    "\n",
    "        ffn_out = self.layernorm_3(ffn_out + out_2, training=training)\n",
    "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
    "        preds = self.out(ffn_out)\n",
    "        return preds\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se genera una clase para encapsular la CNN, el encoder y el decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel(keras.Model):\n",
    "    def __init__(\n",
    "        self, cnn_model, encoder, decoder, num_captions_per_image=CAPTIONS_PER_IMAGE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cnn_model = cnn_model\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "        self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n",
    "        self.num_captions_per_image = num_captions_per_image\n",
    "\n",
    "    def calculate_loss(self, y_true, y_pred, mask):\n",
    "        loss = self.loss(y_true, y_pred)\n",
    "        mask = tf.cast(mask, dtype=loss.dtype)\n",
    "        loss *= mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
    "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
    "        accuracy = tf.math.logical_and(mask, accuracy)\n",
    "        accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
    "\n",
    "    def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):\n",
    "        encoder_out = self.encoder(img_embed, training=training)\n",
    "        batch_seq_inp = batch_seq[:, :-1]\n",
    "        batch_seq_true = batch_seq[:, 1:]\n",
    "        mask = tf.math.not_equal(batch_seq_true, 0)\n",
    "        batch_seq_pred = self.decoder(\n",
    "            batch_seq_inp, encoder_out, training=training, mask=mask\n",
    "        )\n",
    "        loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n",
    "        acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n",
    "        return loss, acc\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        batch_img, batch_seq = batch_data\n",
    "        batch_loss = 0\n",
    "        batch_acc = 0\n",
    "\n",
    "        # 1. Get image embeddings\n",
    "        img_embed = self.cnn_model(batch_img)\n",
    "\n",
    "        # 2. Pass each of the five captions one by one to the decoder\n",
    "        # along with the encoder outputs and compute the loss as well as accuracy\n",
    "        # for each caption.\n",
    "        for i in range(self.num_captions_per_image):\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss, acc = self._compute_caption_loss_and_acc(\n",
    "                    img_embed, batch_seq[:, i, :], training=True\n",
    "                )\n",
    "\n",
    "                # 3. Update loss and accuracy\n",
    "                batch_loss += loss\n",
    "                batch_acc += acc\n",
    "\n",
    "            # 4. Get the list of all the trainable weights\n",
    "            train_vars = (\n",
    "                self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "            )\n",
    "\n",
    "            # 5. Get the gradients\n",
    "            grads = tape.gradient(loss, train_vars)\n",
    "\n",
    "            # 6. Update the trainable weights\n",
    "            self.optimizer.apply_gradients(zip(grads, train_vars))\n",
    "\n",
    "        # 7. Update the trackers\n",
    "        batch_acc /= float(self.num_captions_per_image)\n",
    "        self.loss_tracker.update_state(batch_loss)\n",
    "        self.acc_tracker.update_state(batch_acc)\n",
    "\n",
    "        # 8. Return the loss and accuracy values\n",
    "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "\n",
    "    def test_step(self, batch_data):\n",
    "        batch_img, batch_seq = batch_data\n",
    "        batch_loss = 0\n",
    "        batch_acc = 0\n",
    "\n",
    "        # 1. Get image embeddings\n",
    "        img_embed = self.cnn_model(batch_img)\n",
    "\n",
    "        # 2. Pass each of the five captions one by one to the decoder\n",
    "        # along with the encoder outputs and compute the loss as well as accuracy\n",
    "        # for each caption.\n",
    "        for i in range(self.num_captions_per_image):\n",
    "            loss, acc = self._compute_caption_loss_and_acc(\n",
    "                img_embed, batch_seq[:, i, :], training=False\n",
    "            )\n",
    "\n",
    "            # 3. Update batch loss and batch accuracy\n",
    "            batch_loss += loss\n",
    "            batch_acc += acc\n",
    "\n",
    "        batch_acc /= float(self.num_captions_per_image)\n",
    "\n",
    "        # 4. Update the trackers\n",
    "        self.loss_tracker.update_state(batch_loss)\n",
    "        self.acc_tracker.update_state(batch_acc)\n",
    "\n",
    "        # 5. Return the loss and accuracy values\n",
    "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We need to list our metrics here so the `reset_states()` can be\n",
    "        # called automatically.\n",
    "        return [self.loss_tracker, self.acc_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "cnn_model = get_cnn_model()\n",
    "encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=1)\n",
    "decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=2)\n",
    "caption_model = ImageCaptioningModel(\n",
    "    cnn_model=cnn_model, encoder=encoder, decoder=decoder \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Entrenamiento del modelo\n",
    "\n",
    "---\n",
    "[Regresar a contenido](#general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "cross_entropy = keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False, reduction=\"none\"\n",
    ")\n",
    "\n",
    "# EarlyStopping criteria\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler for the optimizer\n",
    "class LRSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, post_warmup_learning_rate, warmup_steps):\n",
    "        super().__init__()\n",
    "        self.post_warmup_learning_rate = post_warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        global_step = tf.cast(step, tf.float32)\n",
    "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "        warmup_progress = global_step / warmup_steps\n",
    "        warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress\n",
    "        return tf.cond(\n",
    "            global_step < warmup_steps,\n",
    "            lambda: warmup_learning_rate,\n",
    "            lambda: self.post_warmup_learning_rate,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot callback\n",
    "class PlotLearning(keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    Callback to plot the learning curves of the model during training.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.epochs = []\n",
    "        self.times = []\n",
    "        self.timetaken = 0\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.metrics = {}\n",
    "        for metric in logs:\n",
    "            self.metrics[metric] = []\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.timetaken = tf.timestamp()\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Storing metrics\n",
    "        for metric in logs:\n",
    "            if metric in self.metrics:\n",
    "                self.metrics[metric].append(logs.get(metric))\n",
    "            else:\n",
    "                self.metrics[metric] = [logs.get(metric)]\n",
    "                \n",
    "        # Storing time\n",
    "        self.times.append((tf.timestamp() - self.timetaken) / 60)\n",
    "        self.epochs.append(epoch + 1)\n",
    "        \n",
    "        # Plot accuracy and loss per epoch\n",
    "        metrics = [x for x in logs if 'val' not in x]\n",
    "        \n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        fig.set_facecolor('white')\n",
    "        \n",
    "        subfigs = fig.subfigures(2, 1)\n",
    "        \n",
    "        subplots = subfigs[0].subplots(1, len(metrics))\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "\n",
    "        for i, metric in enumerate(metrics):\n",
    "            metricVarName = 'accuracy' if metric == 'acc' else metric\n",
    "            \n",
    "            subplots[i].plot(range(1, epoch + 2), self.metrics[metric], label=metricVarName)\n",
    "            \n",
    "            if logs['val_' + metric]:\n",
    "                subplots[i].plot(range(1, epoch + 2), self.metrics['val_' + metric], \n",
    "                                 label='val_' + metricVarName)\n",
    "            \n",
    "            metricLabel = 'Accuracy' if metric == 'acc' else 'Loss'\n",
    "            \n",
    "            subplots[i].set_title('Model ' + metricLabel)\n",
    "            subplots[i].set_ylabel(metricLabel)\n",
    "            subplots[i].set_xlabel('Epoch')\n",
    "            subplots[i].legend()\n",
    "            subplots[i].grid()\n",
    "            \n",
    "        # Plot time per epoch\n",
    "        subplots = subfigs[1].subplots(1, 1)\n",
    "        \n",
    "        subplots.set_title('Minutes per epoch - Total time ' + \n",
    "                  str(round(np.sum(self.times) / 60, 2)) + 'hrs')\n",
    "        subplots.set_xlabel('Epoch')\n",
    "        subplots.set_ylabel('Time in minutes')\n",
    "        subplots.plot(self.epochs, self.times)\n",
    "        subplots.grid()\n",
    "        \n",
    "        bboxprops = dict(boxstyle='round', facecolor='white', alpha=0.75)\n",
    "        \n",
    "        for i in range(len(self.epochs)):\n",
    "            j = self.times[i].numpy()\n",
    "            subplots.text(i + 1, j, str(round(j, 2)), bbox=bboxprops)\n",
    "        \n",
    "        plt.tight_layout(rect=[-0.05, 0.05, 1, 0.92])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a learning rate schedule\n",
    "num_train_steps = len(train_dataset) * EPOCHS\n",
    "num_warmup_steps = num_train_steps // 15\n",
    "lr_schedule = LRSchedule(post_warmup_learning_rate=1e-3, warmup_steps=num_warmup_steps)\n",
    "\n",
    "# Compile the model\n",
    "caption_model.compile(optimizer=keras.optimizers.Adam(lr_schedule), loss=cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos entrenados para ambos conjuntos de datos se encuentran en la siguiente liga:\n",
    "\n",
    "[Link a Google Drive](https://drive.google.com/drive/folders/1V4gz_EPmrL2KOKl8-WVB8X57VZ4Hfx8q?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model weights\n",
    "print('Loading model weights...')\n",
    "\n",
    "caption_model.load_weights(WEIGHTS_FILE_PATH)\n",
    "\n",
    "print('Model weights loaded from: ' + WEIGHTS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = caption_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[early_stopping, PlotLearning()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights\n",
    "print(\"Saving model weights...\")\n",
    "\n",
    "caption_model.save_weights(WEIGHTS_FILE_PATH, save_format='tf')\n",
    "\n",
    "print('Weights are saved in: ' + WEIGHTS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Pruebas de resultados\n",
    "\n",
    "---\n",
    "[Regresar a contenido](#general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorization.get_vocabulary()\n",
    "index_lookup = dict(zip(range(len(vocab)), vocab))\n",
    "max_decoded_sentence_length = SEQ_LENGTH - 1\n",
    "valid_images = list(valid_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(path, plotImage):\n",
    "    # Read the image from the disk\n",
    "    sample_img = read_valid_image(path)\n",
    "    img = sample_img.numpy().clip(0, 255).astype(np.uint8)\n",
    "    \n",
    "    if plotImage:\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    # Pass the image to the CNN\n",
    "    img = tf.expand_dims(sample_img, 0)\n",
    "    img = caption_model.cnn_model(img)\n",
    "\n",
    "    # Pass the image features to the Transformer encoder\n",
    "    encoded_img = caption_model.encoder(img, training=False)\n",
    "\n",
    "    # Generate the caption using the Transformer decoder\n",
    "    decoded_caption = \"<start> \"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_caption = vectorization([decoded_caption])[:, :-1]\n",
    "        mask = tf.math.not_equal(tokenized_caption, 0)\n",
    "        predictions = caption_model.decoder(\n",
    "            tokenized_caption, encoded_img, training=False, mask=mask\n",
    "        )\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = index_lookup[sampled_token_index]\n",
    "        if sampled_token == \" <end>\":\n",
    "            break\n",
    "        decoded_caption += \" \" + sampled_token\n",
    "\n",
    "    decoded_caption = decoded_caption.replace(\"<start> \", \"\")\n",
    "    decoded_caption = decoded_caption.replace(\" <end>\", \"\").strip()\n",
    "    \n",
    "    # Remove decoded caption duplicates\n",
    "    if (CURRENT_DATABASE == Database.CheXpert):\n",
    "        words_set = set(decoded_caption.split())\n",
    "        decoded_caption = ' '.join(sorted(words_set, key=decoded_caption.index))\n",
    "    \n",
    "    return decoded_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_caption():\n",
    "    image_path = np.random.choice(valid_images)\n",
    "    print(\"Image Path: \" + image_path)\n",
    "    \n",
    "    print(generate_caption(image_path, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption_with_path(path):\n",
    "    print(\"Image Path: \" + path)\n",
    "    \n",
    "    print(generate_caption(path, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_random_caption()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_random_caption('datasets/CheXpert-v1.0-small/train/patient58473/study1/view1_frontal.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Evaluación del modelo\n",
    "\n",
    "---\n",
    "[Regresar a contenido](#general)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizan las pruebas de BLEU y ROUGE para evaluar la precisión de las captions generadas por nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU & ROUGE Score\n",
    "def evaluate_bleu_and_rouge_score(image_paths):\n",
    "    gramScores = [0, 0, 0, 0]\n",
    "    f_p_r_measures = [0, 0, 0]\n",
    "    \n",
    "    bleuWeights = [(1, 0, 0, 0), (0, 1, 0, 0), (1, 0, 1, 0), (0, 0, 0, 1)]\n",
    "    for path, captions in tqdm(image_paths.items()):\n",
    "        test_caption = generate_caption(path, False).split()\n",
    "        hypo_captions = np.array(captions, copy=True).tolist()\n",
    "        \n",
    "        for i in range(len(hypo_captions)):\n",
    "            hypo_captions[i] = hypo_captions[i].replace(\"<start> \", \"\")\n",
    "            hypo_captions[i] = hypo_captions[i].replace(\" <end>\", \"\")\n",
    "            hypo_captions[i] = hypo_captions[i].lower()\n",
    "            hypo_captions[i] = hypo_captions[i].split()\n",
    "        \n",
    "        # BLEU Score\n",
    "        for i in range(len(gramScores)):\n",
    "            gramScores[i] += sentence_bleu(hypo_captions, test_caption, weights=bleuWeights[i])\n",
    "        \n",
    "        # ROUGE Score\n",
    "        result = text.metrics.rouge_l(tf.ragged.constant(hypo_captions), \n",
    "                                    tf.ragged.constant([test_caption]))\n",
    "        \n",
    "        f_p_r_measures[0] += result.f_measure.numpy()[0]\n",
    "        f_p_r_measures[1] += result.p_measure.numpy()[0]\n",
    "        f_p_r_measures[2] += result.r_measure.numpy()[0]\n",
    "    \n",
    "    for i in range(len(gramScores)):\n",
    "        print('BLEU Score ' + str(i+1) + ' gram: ' + str(gramScores[i] / len(image_paths)))\n",
    "    \n",
    "    print()\n",
    "    print('ROUGE F Score: ' + str(f_p_r_measures[0] / len(image_paths)))\n",
    "    print('ROUGE P Score: ' + str(f_p_r_measures[1] / len(image_paths)))\n",
    "    print('ROUGE R Score: ' + str(f_p_r_measures[2] / len(image_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_evaluate = valid_data\n",
    "\n",
    "if (CURRENT_DATABASE == Database.CheXpert):\n",
    "    data_to_evaluate = dict(itertools.islice(valid_data.items(), 5000))\n",
    "\n",
    "# Evaluate valid data BLEU & ROUGE score\n",
    "evaluate_bleu_and_rouge_score(data_to_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"d\"></a>\n",
    "### 4. Conclusiones\n",
    "\n",
    "---\n",
    "[Regresar a contenido](#general)\n",
    "\n",
    "De su análisis, qué pueden concluir? Qué posibilidades extras pudieran aplicarse o con qué finalidad realizaron el trabajo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La limpieza de ambos conjuntos de datos limitó el entrenamiento y las pruebas del modelo, ya que la cantidad de datos no era lo suficientemente amplia. La falta de datos se reflejó en los resultados obtenidos a partir de la métrica BLEU y ROUGE. En la evaluación de MedPix, los valores estaban más cerca de 0 que de 1. Por otro lado, los valores obtenidos con CheXpert estaban más cerca de 1, sin embargo, esto puede ser engañoso porque las pruebas realizadas con el primer conjunto de datos se realizaron utilizando grandes, mixtos, frases en lenguaje natural complejas y no estructuradas, en lugar de etiquetas simples como las que se usan con CheXpert. Es importante tener en cuenta que el entrenamiento y la prueba del modelo con ambos conjuntos de datos tomó varias horas, de hecho, se usó menos de la mitad de los datos en el conjunto de datos de CheXpert debido a esto. Hay una serie de limitaciones potenciales para este proyecto, una de las más importantes es la potencia de la computadora, lo que limita la cantidad de datos que podemos usar y la cantidad de etiquetas que podemos incluir en el modelo.\n",
    "\n",
    "Cuantos más datos o etiquetas usamos para el modelo, más tiempo se tarda en entrenar, lo que lo hace casi insoportable para la potencia informática disponible. Esto afecta fuertemente los resultados de nuestro modelo porque con menos datos la pérdida alcanza un punto donde deja de descender y la precisión comienza a decaer. En cuanto a los resultados generales, la interpretación resultante puede parecer repetitiva y confusa. Más capacitación con datos estructurados y una cantidad limitada de subtítulos podría mejorar la efectividad del modelo. En cuanto a la arquitectura, se diseñó y desarrolló teniendo en cuenta los avances más recientes en el campo, la mayoría de ellos también necesitan mayor potencia de cómputo y junto con ello mayores requerimientos energéticos que deben destacarse por su replicabilidad. Una potencial mejora sería migrar los encoders de texto a unos previamente entrenados para imagenología clínica, entre las posibilidades con mayor potencial están: MedBERT y ClinicalBERT."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
